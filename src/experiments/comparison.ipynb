{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244eeb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import random\n",
    "from dotenv import load_dotenv\n",
    "from mistralai.client import MistralClient\n",
    "from mistralai.models.chat_completion import ChatMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2b59d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb13108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar variables de entorno\n",
    "load_dotenv()\n",
    "client = MistralClient(api_key=\"XEV0fCx3MqiG9HqVkGc4Hy5qyD3WwPHr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cf03ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Cargar el corpus de conocimiento\n",
    "def load_corpus(corpus_path='../data/processed/normalized_data.json'):\n",
    "    \"\"\"Carga el corpus de conocimiento desde un archivo JSON\"\"\"\n",
    "    with open(corpus_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379b6ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones de visualización para el análisis de bootstrapping\n",
    "def plot_bootstrap_distribution(analysis, bootstrap_means):\n",
    "    \"\"\"Genera gráficas para visualizar la distribución del bootstrapping\"\"\"\n",
    "    plt.style.use('seaborn-v0_8')\n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # 1. Distribución de medias bootstrap\n",
    "    plt.subplot(2, 1, 1)\n",
    "    sns.histplot(data=bootstrap_means, bins=30, color='skyblue', alpha=0.6)\n",
    "    plt.axvline(analysis['bootstrap_mean'], color='red', linestyle='--', \n",
    "                label=f'Media Bootstrap ({analysis[\"bootstrap_mean\"]:.2f})')\n",
    "    plt.axvline(analysis['ci_95_lower'], color='green', linestyle=':', \n",
    "                label=f'IC 95% ({analysis[\"ci_95_lower\"]:.2f}, {analysis[\"ci_95_upper\"]:.2f})')\n",
    "    plt.axvline(analysis['ci_95_upper'], color='green', linestyle=':')\n",
    "    plt.title('Distribución de Medias por Bootstrap')\n",
    "    plt.xlabel('Puntuación')\n",
    "    plt.ylabel('Frecuencia')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Gráfica de barras con intervalos de confianza\n",
    "    plt.subplot(2, 1, 2)\n",
    "    comparisons = ['Muestra Original', 'Bootstrap']\n",
    "    means = [analysis['original_mean'], analysis['bootstrap_mean']]\n",
    "    errors = [analysis['original_std'], \n",
    "              (analysis['ci_95_upper'] - analysis['ci_95_lower']) / 2]\n",
    "    \n",
    "    bars = plt.bar(comparisons, means, yerr=errors, capsize=5, \n",
    "                   color=['lightcoral', 'skyblue'], alpha=0.6)\n",
    "    \n",
    "    # Añadir valores sobre las barras\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.2f}',\n",
    "                ha='center', va='bottom')\n",
    "    \n",
    "    plt.title('Comparación de Medias: Original vs Bootstrap')\n",
    "    plt.xlabel('Método')\n",
    "    plt.ylabel('Puntuación')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def plot_evaluation_heatmap(results_df):\n",
    "    \"\"\"Genera un heatmap de puntuaciones por pregunta\"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Crear matriz de puntuaciones (ya es DataFrame, no necesita to_frame())\n",
    "    eval_data = results_df.pivot_table(\n",
    "        index='pregunta',\n",
    "        values='evaluacion',\n",
    "        aggfunc='first'\n",
    "    ).sort_values('evaluacion', ascending=False)\n",
    "    \n",
    "    # Crear heatmap - elimina .to_frame() ya que eval_data ya es DataFrame\n",
    "    sns.heatmap(eval_data,  # <-- Cambio principal aquí\n",
    "                cmap='YlOrRd',\n",
    "                annot=True,\n",
    "                fmt='.2f',\n",
    "                cbar_kws={'label': 'Puntuación'})\n",
    "    \n",
    "    plt.title('Mapa de Calor de Puntuaciones por Pregunta')\n",
    "    plt.ylabel('Preguntas')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e5adc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones de procesamiento de respuestas\n",
    "def extract_content_from_response(response):\n",
    "    \"\"\"Extrae el contenido real de una respuesta que puede contener metadatos\"\"\"\n",
    "    if isinstance(response, str):\n",
    "        return response\n",
    "    return str(response)\n",
    "\n",
    "def clean_response(resp):\n",
    "    \"\"\"Limpia y normaliza una respuesta\"\"\"\n",
    "    if not isinstance(resp, str):\n",
    "        return \"\"\n",
    "    # Limpiar caracteres especiales y espacios extra\n",
    "    cleaned = resp.strip().replace('\\n', ' ').replace('\\r', ' ')\n",
    "    while '  ' in cleaned:\n",
    "        cleaned = cleaned.replace('  ', ' ')\n",
    "    return cleaned\n",
    "\n",
    "def process_response(response):\n",
    "    \"\"\"Procesa una respuesta extrayendo el contenido y limpiándolo\"\"\"\n",
    "    return clean_response(extract_content_from_response(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fdc755",
   "metadata": {},
   "source": [
    "# Procesamiento de Respuestas\n",
    "\n",
    "El notebook ha sido actualizado para manejar correctamente las respuestas del chatbot que incluyen metadatos. Las principales mejoras son:\n",
    "\n",
    "1. **Extracción de contenido**: Se extrae el contenido real de las respuestas que vienen en formato completo de Mistral\n",
    "2. **Limpieza de respuestas**: Se normalizan los espacios y caracteres especiales\n",
    "3. **Evaluación robusta**: La evaluación de similitud ahora maneja mejor diferentes formatos de respuesta\n",
    "4. **Fallback automático**: Si hay errores en la evaluación principal, se usa una métrica de similitud de coseno como respaldo\n",
    "\n",
    "Esto asegura que la comparación se realice solo sobre el contenido relevante de las respuestas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad234675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Generar preguntas basadas en el corpus\n",
    "def generate_corpus_questions(corpus, n=30):\n",
    "    \"\"\"Genera preguntas relevantes basadas en el corpus usando Mistral AI\"\"\"\n",
    "    # Extraer información clave del corpus para construir un contexto\n",
    "    cities = set()\n",
    "    attractions = set()\n",
    "    unique_titles = set()\n",
    "    \n",
    "    for entry in corpus:\n",
    "        cities.add(entry['city'])\n",
    "        if entry.get('attractions'):\n",
    "            attractions.update(entry['attractions'])\n",
    "        if entry.get('title'):\n",
    "            # Limpiar título eliminando caracteres especiales\n",
    "            title = entry['title'].replace('\\r', '').replace('\\n', '').replace('\\t', '').strip()\n",
    "            if title and len(title) < 100:  # Filtrar títulos muy largos\n",
    "                unique_titles.add(title)\n",
    "    \n",
    "    # Construir contexto para el prompt\n",
    "    context = f\"\"\"\n",
    "    Estás generando preguntas turísticas sobre Cuba basadas en este contexto:\n",
    "    - Ciudades mencionadas: {', '.join(cities)}\n",
    "    - Atracciones turísticas: {', '.join(attractions) if attractions else 'No especificadas'}\n",
    "    - Lugares destacados: {', '.join(unique_titles) if unique_titles else 'No especificados'}\n",
    "    \n",
    "    Genera preguntas que:\n",
    "    1. Sean relevantes para la información en el corpus\n",
    "    2. Cubran diferentes aspectos del turismo en Cuba\n",
    "    3. Sean específicas pero naturales (como las haría un turista)\n",
    "    4. Incluyan referencias a ciudades, atracciones y conceptos del contexto\n",
    "    5. Referentes a estos temas solamente: gastronomía, vida nocturna, \n",
    "    lugares históricos y alojamiento\n",
    "    \"\"\"\n",
    "    \n",
    "    # Crear prompt para Mistral AI\n",
    "    prompt = f\"\"\"\n",
    "    {context}\n",
    "    \n",
    "    Genera EXACTAMENTE {n} preguntas sobre turismo en Cuba usando este formato:\n",
    "    [PREGUNTA 1]\n",
    "    [PREGUNTA 2]\n",
    "    ...\n",
    "    [PREGUNTA {n}]\n",
    "    \n",
    "    Reglas:\n",
    "    - Solo incluye la pregunta sin numeración\n",
    "    - Usa diferentes tipos de preguntas (qué, dónde, cómo)\n",
    "    - Refiérete específicamente a: {', '.join(list(cities)[:5])}...\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(\"generando preguntas...\")\n",
    "        # Obtener respuesta de Mistral AI\n",
    "        response = client.chat(\n",
    "            model=\"mistral-large-latest\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        \n",
    "        print(\"preguntas generadas...\")\n",
    "        \n",
    "        # Procesar las preguntas generadas\n",
    "        content = response.choices[0].message.content\n",
    "        questions = []\n",
    "        \n",
    "        for line in content.split('\\n'):\n",
    "            clean_line = line.strip()\n",
    "            if clean_line and clean_line.endswith('?'):\n",
    "                questions.append(clean_line)\n",
    "        \n",
    "        # Asegurarnos de tener exactamente n preguntas\n",
    "        if len(questions) >= n:\n",
    "            return questions[:n]\n",
    "        else:\n",
    "            # Generar preguntas de respaldo si no hay suficientes\n",
    "            base_questions = []\n",
    "            return base_questions[:n]\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error generando preguntas: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21709d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_with_bootstrapping(results, n_bootstraps=1000, sample_size=300):\n",
    "    \"\"\"Realiza análisis estadístico con bootstrapping y genera visualizaciones\"\"\"\n",
    "    quality_scores = [res['evaluacion'] for res in results]\n",
    "    \n",
    "    # Estadísticas originales\n",
    "    original_mean = np.mean(quality_scores)\n",
    "    original_std = np.std(quality_scores)\n",
    "    \n",
    "    # Bootstrapping para simular muestras más grandes\n",
    "    bootstrap_means = []\n",
    "    for _ in range(n_bootstraps):\n",
    "        sample = np.random.choice(quality_scores, size=sample_size, replace=True)\n",
    "        bootstrap_means.append(np.mean(sample))\n",
    "    \n",
    "    # Intervalo de confianza\n",
    "    ci_lower = np.percentile(bootstrap_means, 2.5)\n",
    "    ci_upper = np.percentile(bootstrap_means, 97.5)\n",
    "    \n",
    "    analysis = {\n",
    "        \"original_mean\": original_mean,\n",
    "        \"original_std\": original_std,\n",
    "        \"bootstrap_mean\": np.mean(bootstrap_means),\n",
    "        \"bootstrap_std\": np.std(bootstrap_means),\n",
    "        \"ci_95_lower\": ci_lower,\n",
    "        \"ci_95_upper\": ci_upper,\n",
    "        \"n_bootstraps\": n_bootstraps,\n",
    "        \"sample_size\": sample_size\n",
    "    }\n",
    "    \n",
    "    # Generar visualizaciones\n",
    "    fig_dist = plot_bootstrap_distribution(analysis, bootstrap_means)\n",
    "    \n",
    "    # Crear DataFrame para el heatmap\n",
    "    results_df = pd.DataFrame(results)\n",
    "    fig_heat = plot_evaluation_heatmap(results_df)\n",
    "    \n",
    "    # Guardar las gráficas\n",
    "    timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    fig_dist.savefig(f'bootstrap_distribution_{timestamp}.png', dpi=300, bbox_inches='tight')\n",
    "    fig_heat.savefig(f'quality_heatmap_{timestamp}.png', dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    print(\"\\n=== Resultados del Análisis ===\")\n",
    "    print(f\"Muestra Original (n={len(quality_scores)}):\")\n",
    "    print(f\"- Media: {original_mean:.2f}/5\")\n",
    "    print(f\"- Desviación Estándar: {original_std:.2f}\")\n",
    "    print(f\"\\nBootstrap ({n_bootstraps} iteraciones de tamaño {sample_size}):\")\n",
    "    print(f\"- Media Estimada: {analysis['bootstrap_mean']:.2f}/5\")\n",
    "    print(f\"- Intervalo de Confianza 95%: [{ci_lower:.2f}, {ci_upper:.2f}]\")\n",
    "    \n",
    "    return analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "ef768cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Evaluación de similitud semántica\n",
    "def calculate_precision(query, response):\n",
    "    \"\"\"Evalúa la puntuacion respuestas\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Como experto en turismo en Cuba, analiza esta respuesta a la pregunta dada sobre turismo en Cuba:\n",
    "    \n",
    "    Pregunta: {query}\n",
    "    Respuesta: {response}\n",
    "    \n",
    "    Evalúa considerando:\n",
    "    1. Precisión de la información\n",
    "    2. Coherencia y relevancia\n",
    "    3. Cobertura del tema\n",
    "    4. Calidad de las recomendaciones\n",
    "    5. Ajuste a la pregunta realizada\n",
    "    \n",
    "    Asigna una puntuación:\n",
    "    - 0: Respuesta totalmente errónea\n",
    "    - 1: Mucha alucinación e invención, pero pocos datos reales\n",
    "    - 2: Alguna parte de la información es real y cierta, aunque sigue alucinando\n",
    "    - 3: Bastante de lo que dice es cierto, pero le falta calidad en la respuesta\n",
    "    - 4: Mucho de lo que dice es cierto, tiene buenas recomendaciones, pero aún necesita abarcar un poco más\n",
    "    - 5: Todo o casi todo tiene sentido, es cierto y además es bastante completa la información, incluyendo \n",
    "    recomendaciones buenas\n",
    "\n",
    "    Devuelve SOLO un número entre 0 y 5.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat(\n",
    "            model=\"mistral-large-latest\",\n",
    "            messages=[ChatMessage(role=\"user\", content=prompt)],\n",
    "            max_tokens=10\n",
    "        )\n",
    "        return float(response.choices[0].message.content.strip())\n",
    "    except:\n",
    "        # Fallback: similitud de coseno básica\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        vectorizer = TfidfVectorizer().fit_transform([response])\n",
    "        return ((vectorizer * vectorizer.T).A[0,1] * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "157199b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Flujo principal de evaluación\n",
    "def run_evaluation(corpus_path='corpus.json', n_questions=30):\n",
    "    \"\"\"Ejecuta el proceso completo de evaluación\"\"\"\n",
    "    # Verificar si ya existen preguntas\n",
    "    try:\n",
    "        existing_questions = pd.read_csv('questions.csv')\n",
    "        if not existing_questions.empty:\n",
    "            questions = existing_questions['pregunta'].tolist()\n",
    "            print(f\"Se encontraron {len(questions)} preguntas existentes en questions.csv\")\n",
    "        else:\n",
    "            raise FileNotFoundError\n",
    "    except:\n",
    "        print(\"Generando nuevas preguntas...\")\n",
    "        corpus = load_corpus(corpus_path)\n",
    "        questions = generate_corpus_questions(corpus, n_questions)\n",
    "        pd.DataFrame({'pregunta': questions}).to_csv('questions.csv', index=False)\n",
    "        print(f\"Se generaron {len(questions)} preguntas nuevas y se guardaron en questions.csv\")\n",
    "    \n",
    "    # Función para extraer el contenido de la respuesta\n",
    "    def extract_response_content(response_str):\n",
    "        try:\n",
    "            if 'content=' in response_str:\n",
    "                # Buscar el contenido entre content=' y ', name=\n",
    "                start = response_str.find(\"content='\") + 9\n",
    "                end = response_str.find(\"', name=\")\n",
    "                if start > 8 and end > start:  # Si encontramos ambos marcadores\n",
    "                    return response_str[start:end]\n",
    "            return response_str\n",
    "        except:\n",
    "            return response_str\n",
    "\n",
    "    # Si ya tenemos respuestas manuales, continuar con la evaluación\n",
    "    try:\n",
    "        # Cargar desde JSON\n",
    "        with open('responses.json', 'r', encoding='utf-8') as f:\n",
    "            results_data = json.load(f)\n",
    "        \n",
    "        # Convertir a DataFrame\n",
    "        manual_df = pd.DataFrame(results_data)\n",
    "        \n",
    "        # Verificar consistencia\n",
    "        if len(manual_df) != len(questions):\n",
    "            raise ValueError(f\"Número de respuestas ({len(manual_df)}) no coincide con preguntas ({len(questions)})\")\n",
    "        \n",
    "        # Verificar estructura\n",
    "        if 'pregunta' not in manual_df.columns or 'respuesta' not in manual_df.columns:\n",
    "            raise ValueError(\"Formato JSON inválido: faltan campos requeridos\")\n",
    "            \n",
    "        # Procesar respuestas (si es necesario)\n",
    "        if 'respuesta' in manual_df.columns:  # Para compatibilidad con versiones anteriores\n",
    "            manual_df['respuesta'] = manual_df['respuesta'].apply(extract_response_content)\n",
    "        else:\n",
    "            manual_df['respuesta'] = manual_df['respuesta'].apply(extract_response_content)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando respuestas: {str(e)}\")\n",
    "        # Crear plantilla nueva en formato JSON\n",
    "        results_data = [\n",
    "            {\"pregunta\": q, \"respuesta\": \"\"} \n",
    "            for q in questions\n",
    "        ]\n",
    "        \n",
    "        # Guardar plantilla\n",
    "        with open('responses.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(results_data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(\"Se creó nueva plantilla en 'responses.json'\")\n",
    "        return\n",
    "    \n",
    "    results = []\n",
    "    print(\"\\nIniciando evaluación con Mistral AI...\")\n",
    "    \n",
    "    for i, row in manual_df.iterrows():\n",
    "        max_retries = 3\n",
    "        retry_delay = 2\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                print(f\"Procesando pregunta {i+1}/{len(manual_df)}\")\n",
    "                question = row['pregunta']\n",
    "                response = row['respuesta'].strip()\n",
    "                \n",
    "                if not response:\n",
    "                    print(f\"Advertencia: Respuesta vacía para pregunta {i+1}\")\n",
    "                    continue\n",
    "                \n",
    "                evaluation = calculate_precision(question, response)\n",
    "                \n",
    "                results.append({\n",
    "                    'pregunta': question,\n",
    "                    'respuesta': response,\n",
    "                    'evaluacion': evaluation\n",
    "                })\n",
    "                \n",
    "                # Rate limiting adaptativo\n",
    "                time.sleep(2.0)  # Base delay\n",
    "                break  # Si llegamos aquí, todo salió bien\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error en pregunta {i+1}, intento {attempt + 1}: {str(e)}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(retry_delay * (attempt + 1))  # Backoff exponencial\n",
    "                else:\n",
    "                    print(f\"Error máximo de intentos para pregunta {i+1}\")\n",
    "    \n",
    "    # Análisis estadístico\n",
    "    analysis = analyze_with_bootstrapping(results)\n",
    "    \n",
    "    # Guardar resultados completos\n",
    "    timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(f\"resultados_completos_{timestamp}.csv\", index=False)\n",
    "    \n",
    "    # Guardar análisis estadístico\n",
    "    analysis_df = pd.DataFrame([analysis])\n",
    "    analysis_df.to_csv(f\"analisis_estadistico_{timestamp}.csv\", index=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"RESULTADOS FINALES\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Calidad promedio (30 preguntas): {analysis['original_mean']:.2f} ± {analysis['original_std']:.2f}\")\n",
    "    print(f\"Calidad estimada ({analysis['sample_size']} preguntas simuladas): {analysis['bootstrap_mean']:.2f}\")\n",
    "    print(f\"Intervalo de confianza 95%: [{analysis['ci_95_lower']:.2f}, {analysis['ci_95_upper']:.2f}]\")\n",
    "    \n",
    "    return results_df, analysis_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56088ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar la evaluación\n",
    "# NOTA: La primera ejecución genera el CSV para respuestas manuales\n",
    "# La segunda ejecución (después de completar el CSV) realiza la evaluación completa\n",
    "results, analysis = run_evaluation(corpus_path='../data/processed/normalized_data.json', n_questions=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
