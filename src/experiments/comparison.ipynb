{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb2a9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from dotenv import load_dotenv\n",
    "from mistralai.client import MistralClient\n",
    "from mistralai.models.chat_completion import ChatMessage\n",
    "\n",
    "# Cargar variables de entorno\n",
    "load_dotenv()\n",
    "client = MistralClient(api_key=\"XEV0fCx3MqiG9HqVkGc4Hy5qyD3WwPHr\")  # Inicializar cliente Mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f72fbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chatbot.core import CubaChatbot\n",
    "from chatbot.gap_detector import GapDetector\n",
    "from crawlers.dynamic_crawler import DynamicCrawler\n",
    "from agents.retriever_agent import RetrieverAgent\n",
    "from agents.generator_agent import GeneratorAgent\n",
    "from agents.gap_detector_agent import GapDetectorAgent\n",
    "from agents.updater_agent import UpdaterAgent\n",
    "from agents.agent_manager import AgentManager\n",
    "from agents.guide_agent import GuideAgent\n",
    "from agents.planner_agent import TravelPlannerAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508cb053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mover las inicializaciones dentro de las funciones que las necesitan\n",
    "def get_chatbot():\n",
    "    if 'chatbot' not in globals():\n",
    "        global chatbot\n",
    "        chatbot = CubaChatbot()\n",
    "        if not chatbot.vector_db.get_documents():\n",
    "            chatbot.vector_db.reload_data()\n",
    "            \n",
    "    return chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32159ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Funci√≥n para tu chatbot\n",
    "def your_chatbot_response(question):\n",
    "    \"\"\"\n",
    "    Procesa una pregunta y devuelve la respuesta del chatbot\n",
    "    :param question: Pregunta del usuario\n",
    "    :return: Respuesta del chatbot\n",
    "    \"\"\"\n",
    "    # chatbot = get_chatbot()\n",
    "    \n",
    "    chatbot = CubaChatbot()\n",
    "    if not chatbot.vector_db.get_documents():\n",
    "        print(\"Cargando datos iniciales...\")\n",
    "        try:\n",
    "            chatbot.vector_db.reload_data()\n",
    "            if not chatbot.vector_db.get_documents():\n",
    "                print(\"Error cr√≠tico al cargar datos\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error cr√≠tico al cargar datos: {str(e)}\")\n",
    "            raise RuntimeError(\"No se pudieron cargar los datos iniciales\") from e\n",
    "\n",
    "    # Inicializar componentes\n",
    "    detector = GapDetector(chatbot.vector_db)\n",
    "    updater = DynamicCrawler()\n",
    "    guide_agent = GuideAgent(chatbot.vector_db)\n",
    "    planner_agent = TravelPlannerAgent(chatbot.vector_db)\n",
    "\n",
    "    # Crear agentes\n",
    "    retriever_agent = RetrieverAgent(chatbot.vector_db)\n",
    "    generator_agent = GeneratorAgent(guide_agent, planner_agent)\n",
    "    gap_detector_agent = GapDetectorAgent(detector)\n",
    "    updater_agent = UpdaterAgent(updater)\n",
    "\n",
    "    # Manager de agentes\n",
    "    manager = AgentManager([\n",
    "        retriever_agent,\n",
    "        generator_agent,\n",
    "        gap_detector_agent,\n",
    "        updater_agent\n",
    "    ])\n",
    "    \n",
    "    # Recuperar contexto relevante\n",
    "    retrieval_task = {\"type\": \"retrieve\", \"query\": question}\n",
    "    context = manager.dispatch(retrieval_task, {})\n",
    "    \n",
    "    # Generar respuesta inicial\n",
    "    generate_task = {\"type\": \"generate\", \"prompt\": question}\n",
    "    response = manager.dispatch(generate_task, context)\n",
    "    \n",
    "    # Convertir respuesta a texto si es necesario\n",
    "    if hasattr(response, \"choices\"):\n",
    "        response_text = \" \".join([choice.message.content for choice in response.choices])\n",
    "    else:\n",
    "        response_text = str(response)\n",
    " \n",
    "    detect_task = {\"type\": \"detect_gap\", \"prompt\": question, \"response\": response_text}\n",
    "    needs_update = manager.dispatch(detect_task, context)\n",
    "    \n",
    "    if needs_update:\n",
    "        print(\"üîÑ Actualizando informaci√≥n...\")\n",
    "        sources = detector.identify_outdated_sources(question)\n",
    "        update_task = {\"type\": \"update_sources\", \"sources\": sources}\n",
    "        manager.dispatch(update_task, context)\n",
    "        chatbot.vector_db.reload_data()\n",
    "        print(\"‚úÖ Actualizaci√≥n completada\")\n",
    "        \n",
    "        # Regenerar respuesta con datos actualizados\n",
    "        response = manager.dispatch(generate_task, context)\n",
    "        if hasattr(response, \"choices\"):\n",
    "            response_text = \" \".join([choice.message.content for choice in response.choices])\n",
    "        else:\n",
    "            response_text = str(response)\n",
    "    \n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e989152f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Chatbot alternativo (ahora usando Mistral AI)\n",
    "def alternative_chatbot_response(question):\n",
    "    \"\"\"Chatbot alternativo especializado en turismo cubano usando Mistral\"\"\"\n",
    "    system_prompt = \"\"\"\n",
    "    Eres un asistente de viajes especializado en Cuba. Responde preguntas sobre:\n",
    "    - Destinos tur√≠sticos populares y menos conocidos\n",
    "    - Recomendaciones de itinerarios\n",
    "    - Informaci√≥n cultural e hist√≥rica\n",
    "    - Consejos pr√°cticos para viajeros\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat(\n",
    "            model=\"mistral-large-latest\",  # Modelo de Mistral\n",
    "            messages=[\n",
    "                ChatMessage(role=\"system\", content=system_prompt),\n",
    "                ChatMessage(role=\"user\", content=question)\n",
    "            ],\n",
    "            temperature=0.5,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error en chatbot alternativo: {str(e)}\")\n",
    "        return \"Error: No se pudo generar respuesta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e344a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Generaci√≥n de preguntas tur√≠sticas con Mistral\n",
    "def generate_tourism_questions(n=10, country=\"Cuba\"):\n",
    "    \"\"\"Genera preguntas tur√≠sticas usando Mistral\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Genera {n} preguntas espec√≠ficas sobre turismo en {country} que cubran:\n",
    "    - Destinos menos conocidos\n",
    "    - Rutas de senderismo\n",
    "    - Recomendaciones gastron√≥micas locales\n",
    "    - Eventos culturales\n",
    "    - Opciones de ecoturismo\n",
    "    - Transporte entre ciudades\n",
    "    - Requisitos de viaje\n",
    "    - Itinerarios personalizados\n",
    "    \n",
    "    Formato: Lista numerada\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat(\n",
    "            model=\"mistral-large-latest\",\n",
    "            messages=[ChatMessage(role=\"user\", content=prompt)],\n",
    "            temperature=0.7,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        \n",
    "        # Parsear la respuesta\n",
    "        content = response.choices[0].message.content\n",
    "        questions = []\n",
    "        for line in content.split(\"\\n\"):\n",
    "            if line.strip() and (line[:2].strip().isdigit() or \"?\" in line):\n",
    "                # Eliminar n√∫meros y puntos iniciales\n",
    "                clean_line = line.split(\". \", 1)[-1] if \". \" in line else line\n",
    "                questions.append(clean_line.strip())\n",
    "                \n",
    "        return questions[:n]\n",
    "    except Exception as e:\n",
    "        print(f\"Error generando preguntas: {str(e)}\")\n",
    "        return [\n",
    "            \"¬øCu√°les son las mejores playas para familias en Cuba?\",\n",
    "            \"¬øQu√© documentos necesito para viajar a Cuba desde Colombia?\",\n",
    "            \"Recomi√©ndame un itinerario de 7 d√≠as en La Habana\",\n",
    "            \"¬øD√≥nde puedo disfrutar de la mejor m√∫sica cubana aut√©ntica?\",\n",
    "            \"¬øEs seguro viajar por cuenta propia en Cuba?\"\n",
    "        ][:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a6b418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Evaluaci√≥n comparativa con Mistral\n",
    "def evaluate_responses(question, response_a, response_b):\n",
    "    \"\"\"Eval√∫a las respuestas usando Mistral\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Eval√∫a las dos respuestas a la pregunta tur√≠stica sobre Cuba usando estos criterios:\n",
    "    1. Precisi√≥n de la informaci√≥n (0-3 puntos)\n",
    "    2. Relevancia para el turista (0-2 puntos)\n",
    "    3. Utilidad pr√°ctica (0-2 puntos)\n",
    "    4. Riqueza de detalles espec√≠ficos (0-2 puntos)\n",
    "    5. Claridad en la presentaci√≥n (0-1 punto)\n",
    "    \n",
    "    Pregunta: {question}\n",
    "    \n",
    "    Respuesta A: {response_a}\n",
    "    Respuesta B: {response_b}\n",
    "    \n",
    "    Proporciona el puntaje total para cada respuesta (0-10) en formato JSON:\n",
    "    {{\n",
    "        \"score_A\": 0-10,\n",
    "        \"score_B\": 0-10,\n",
    "        \"explanation\": \"Breve justificaci√≥n\"\n",
    "    }}\n",
    "    \n",
    "    Solo devuelve el objeto JSON, sin texto adicional.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat(\n",
    "            model=\"mistral-large-latest\",\n",
    "            messages=[ChatMessage(role=\"user\", content=prompt)],\n",
    "            temperature=0.0,\n",
    "            max_tokens=300\n",
    "        )\n",
    "        \n",
    "        # Extraer y limpiar el JSON de la respuesta\n",
    "        json_str = response.choices[0].message.content\n",
    "        json_str = json_str.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "        evaluation = json.loads(json_str)\n",
    "        return evaluation\n",
    "    except Exception as e:\n",
    "        print(f\"Error en evaluaci√≥n: {str(e)}\")\n",
    "        return {\"score_A\": 5, \"score_B\": 5, \"explanation\": \"Error en evaluaci√≥n\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f978d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. An√°lisis estad√≠stico\n",
    "def analyze_results(results):\n",
    "    scores_a = [res[\"score_A\"] for res in results]\n",
    "    scores_b = [res[\"score_B\"] for res in results]\n",
    "    \n",
    "    # Estad√≠sticas descriptivas\n",
    "    mean_a = np.mean(scores_a)\n",
    "    mean_b = np.mean(scores_b)\n",
    "    std_a = np.std(scores_a)\n",
    "    std_b = np.std(scores_b)\n",
    "    \n",
    "    # Prueba t pareada\n",
    "    t_stat, p_value = stats.ttest_rel(scores_a, scores_b)\n",
    "    \n",
    "    # Proporci√≥n de victorias\n",
    "    wins_a = sum(1 for i in range(len(scores_a)) if scores_a[i] > scores_b[i])\n",
    "    wins_b = sum(1 for i in range(len(scores_a)) if scores_b[i] > scores_a[i])\n",
    "    ties = len(scores_a) - wins_a - wins_b\n",
    "    \n",
    "    # Tama√±o del efecto (Cohen's d)\n",
    "    d = (mean_a - mean_b) / np.sqrt((std_a**2 + std_b**2) / 2)\n",
    "    \n",
    "    return {\n",
    "        \"mean_score_A\": mean_a,\n",
    "        \"mean_score_B\": mean_b,\n",
    "        \"std_score_A\": std_a,\n",
    "        \"std_score_B\": std_b,\n",
    "        \"p_value\": p_value,\n",
    "        \"wins_A\": wins_a,\n",
    "        \"wins_B\": wins_b,\n",
    "        \"ties\": ties,\n",
    "        \"effect_size\": d\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487aab99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flujo principal de experimentaci√≥n\n",
    "def run_experiment(num_questions=10):\n",
    "    \"\"\"Ejecuta el experimento completo\"\"\"\n",
    "    # Generar preguntas\n",
    "    questions = generate_tourism_questions(num_questions, \"Cuba\")\n",
    "    results = []\n",
    "    \n",
    "    print(f\"\\n{'='*50}\\nIniciando experimento con {len(questions)} preguntas\\n{'='*50}\")\n",
    "    \n",
    "    for i, question in enumerate(questions):\n",
    "        print(f\"\\nPregunta {i+1}/{len(questions)}: {question}\")\n",
    "        \n",
    "        # Obtener respuestas\n",
    "        your_response = your_chatbot_response(question)\n",
    "        alt_response = alternative_chatbot_response(question)\n",
    "        \n",
    "        print(f\"\\nTu respuesta ({len(your_response)} caracteres):\\n{your_response[:200]}...\")\n",
    "        print(f\"\\nRespuesta alternativa ({len(alt_response)} caracteres):\\n{alt_response[:200]}...\")\n",
    "        \n",
    "        # Evaluar\n",
    "        evaluation = evaluate_responses(question, your_response, alt_response)\n",
    "        results.append({\n",
    "            \"question\": question,\n",
    "            \"your_response\": your_response,\n",
    "            \"alternative_response\": alt_response,\n",
    "            **evaluation\n",
    "        })\n",
    "        \n",
    "        print(f\"\\nEvaluaci√≥n: Tu chatbot {evaluation['score_A']} vs Alternativo {evaluation['score_B']}\")\n",
    "        print(f\"Explicaci√≥n: {evaluation['explanation']}\")\n",
    "        \n",
    "        # Esperar para evitar rate limits\n",
    "        wait_time = 15 if i % 3 == 0 else 5\n",
    "        print(f\"Esperando {wait_time} segundos...\")\n",
    "        time.sleep(wait_time)\n",
    "    \n",
    "    # An√°lisis final\n",
    "    analysis = analyze_results(results)\n",
    "    \n",
    "    print(f\"\\n{'='*50}\\nRESULTADOS FINALES\\n{'='*50}\")\n",
    "    print(f\"Puntuaci√≥n promedio Tu Chatbot: {analysis['mean_score_A']:.2f} ¬± {analysis['std_score_A']:.2f}\")\n",
    "    print(f\"Puntuaci√≥n promedio Alternativo: {analysis['mean_score_B']:.2f} ¬± {analysis['std_score_B']:.2f}\")\n",
    "    print(f\"p-value: {analysis['p_value']:.5f}\")\n",
    "    print(f\"Victorias: Tu Chatbot {analysis['wins_A']} - Alternativo {analysis['wins_B']} (Empates: {analysis['ties']})\")\n",
    "    print(f\"Tama√±o del efecto (Cohen's d): {analysis['effect_size']:.2f}\")\n",
    "    \n",
    "    # Interpretaci√≥n\n",
    "    if analysis['p_value'] < 0.05:\n",
    "        if analysis['mean_score_A'] > analysis['mean_score_B']:\n",
    "            print(\"CONCLUSI√ìN: Tu chatbot es significativamente mejor (p < 0.05)\")\n",
    "        else:\n",
    "            print(\"CONCLUSI√ìN: El chatbot alternativo es significativamente mejor (p < 0.05)\")\n",
    "    else:\n",
    "        print(\"CONCLUSI√ìN: No hay diferencia significativa entre los chatbots\")\n",
    "    \n",
    "    # Guardar resultados\n",
    "    timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    filename = f\"experiment_results_{timestamp}.csv\"\n",
    "    pd.DataFrame(results).to_csv(filename, index=False)\n",
    "    print(f\"\\nResultados guardados en {filename}\")\n",
    "    \n",
    "    return results, analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f28e7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar experimento\n",
    "results, analysis = run_experiment(num_questions=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
