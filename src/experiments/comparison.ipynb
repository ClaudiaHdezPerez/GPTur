{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "244eeb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import random\n",
    "from dotenv import load_dotenv\n",
    "from mistralai.client import MistralClient\n",
    "from mistralai.models.chat_completion import ChatMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6eb13108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar variables de entorno\n",
    "load_dotenv()\n",
    "client = MistralClient(api_key=\"XEV0fCx3MqiG9HqVkGc4Hy5qyD3WwPHr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6cf03ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Cargar el corpus de conocimiento\n",
    "def load_corpus(corpus_path='../data/processed/normalized_data.json'):\n",
    "    \"\"\"Carga el corpus de conocimiento desde un archivo JSON\"\"\"\n",
    "    with open(corpus_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a561c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. Generar preguntas basadas en el corpus\n",
    "def generate_corpus_questions(corpus, n=30):\n",
    "    \"\"\"Genera preguntas relevantes basadas en el corpus usando Mistral AI\"\"\"\n",
    "    # Extraer información clave del corpus para construir un contexto\n",
    "    cities = set()\n",
    "    attractions = set()\n",
    "    unique_titles = set()\n",
    "    \n",
    "    for entry in corpus:\n",
    "        cities.add(entry['city'])\n",
    "        if entry.get('attractions'):\n",
    "            attractions.update(entry['attractions'])\n",
    "        if entry.get('title'):\n",
    "            # Limpiar título eliminando caracteres especiales\n",
    "            title = entry['title'].replace('\\r', '').replace('\\n', '').replace('\\t', '').strip()\n",
    "            if title and len(title) < 100:  # Filtrar títulos muy largos\n",
    "                unique_titles.add(title)\n",
    "    \n",
    "    # Construir contexto para el prompt\n",
    "    context = f\"\"\"\n",
    "    Estás generando preguntas turísticas sobre Cuba basadas en este contexto:\n",
    "    - Ciudades mencionadas: {', '.join(cities)}\n",
    "    - Atracciones turísticas: {', '.join(attractions) if attractions else 'No especificadas'}\n",
    "    - Lugares destacados: {', '.join(unique_titles) if unique_titles else 'No especificados'}\n",
    "    \n",
    "    Genera preguntas que:\n",
    "    1. Sean relevantes para la información en el corpus\n",
    "    2. Cubran diferentes aspectos del turismo en Cuba\n",
    "    3. Sean específicas pero naturales (como las haría un turista)\n",
    "    4. Incluyan referencias a ciudades, atracciones y conceptos del contexto\n",
    "    \"\"\"\n",
    "    \n",
    "    # Crear prompt para Mistral AI\n",
    "    prompt = f\"\"\"\n",
    "    {context}\n",
    "    \n",
    "    Genera EXACTAMENTE {n} preguntas sobre turismo en Cuba usando este formato:\n",
    "    [PREGUNTA 1]\n",
    "    [PREGUNTA 2]\n",
    "    ...\n",
    "    [PREGUNTA {n}]\n",
    "    \n",
    "    Reglas:\n",
    "    - Solo incluye la pregunta sin numeración\n",
    "    - Usa diferentes tipos de preguntas (qué, dónde, cómo)\n",
    "    - Refiérete específicamente a: {', '.join(list(cities)[:5])}...\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(\"generando preguntas...\")\n",
    "        # Obtener respuesta de Mistral AI\n",
    "        response = client.chat(\n",
    "            model=\"mistral-large-latest\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        \n",
    "        print(\"preguntas generadas...\")\n",
    "        \n",
    "        # Procesar las preguntas generadas\n",
    "        content = response.choices[0].message.content\n",
    "        questions = []\n",
    "        \n",
    "        for line in content.split('\\n'):\n",
    "            clean_line = line.strip()\n",
    "            if clean_line and clean_line.endswith('?'):\n",
    "                questions.append(clean_line)\n",
    "        \n",
    "        # Asegurarnos de tener exactamente n preguntas\n",
    "        if len(questions) >= n:\n",
    "            return questions[:n]\n",
    "        else:\n",
    "            # Generar preguntas de respaldo si no hay suficientes\n",
    "            base_questions = []\n",
    "            return base_questions[:n]\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error generando preguntas: {str(e)}\")\n",
    "\n",
    "# 3. Función para obtener respuestas de tu chatbot\n",
    "def get_your_chatbot_response(question):\n",
    "    \"\"\"\n",
    "    Obtiene respuesta de TU chatbot (ejecutado manualmente)\n",
    "    IMPORTANTE: Esta función asume que ejecutarás manualmente tu chatbot\n",
    "    para las preguntas generadas y guardarás las respuestas en un CSV\n",
    "    \"\"\"\n",
    "    # En la práctica, esto se llenaría manualmente\n",
    "    return \"RESPUESTA DE TU CHATBOT AQUÍ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9e5adc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones de procesamiento de respuestas\n",
    "def extract_content_from_response(response):\n",
    "    \"\"\"Extrae el contenido real de una respuesta que puede contener metadatos\"\"\"\n",
    "    if isinstance(response, str):\n",
    "        if 'content=' in response:\n",
    "            # Buscar el contenido entre content=' y ', name=\n",
    "            start = response.find(\"content='\") + 9\n",
    "            end = response.find(\"', name=\")\n",
    "            if start > 8 and end > start:\n",
    "                return response[start:end]\n",
    "        return response\n",
    "    return str(response)\n",
    "\n",
    "def clean_response(resp):\n",
    "    \"\"\"Limpia y normaliza una respuesta\"\"\"\n",
    "    if not isinstance(resp, str):\n",
    "        return \"\"\n",
    "    # Limpiar caracteres especiales y espacios extra\n",
    "    cleaned = resp.strip().replace('\\n', ' ').replace('\\r', ' ')\n",
    "    while '  ' in cleaned:\n",
    "        cleaned = cleaned.replace('  ', ' ')\n",
    "    return cleaned\n",
    "\n",
    "def process_response(response):\n",
    "    \"\"\"Procesa una respuesta extrayendo el contenido y limpiándolo\"\"\"\n",
    "    return clean_response(extract_content_from_response(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ee19c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de evaluación de similitud actualizada\n",
    "def evaluate_similarity(response_a, response_b):\n",
    "    \"\"\"Evalúa la similitud semántica entre dos respuestas\"\"\"\n",
    "    # Procesar ambas respuestas\n",
    "    response_a = process_response(response_a)\n",
    "    response_b = process_response(response_b)\n",
    "    \n",
    "    if not response_a or not response_b:\n",
    "        return 0  # Si alguna respuesta está vacía, la similitud es 0\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Evalúa la similitud semántica entre estas dos respuestas sobre turismo en Cuba:\n",
    "    - Escala: 0 (completamente diferentes) a 10 (idénticas en significado)\n",
    "    - Considera equivalencia conceptual, no textual\n",
    "    - Ignora diferencias de formato, estilo o idioma\n",
    "    - Enfócate en la información turística proporcionada\n",
    "    \n",
    "    Respuesta 1: {response_a}\n",
    "    Respuesta 2: {response_b}\n",
    "    \n",
    "    Devuelve SOLO un número entre 0 y 10, sin texto adicional.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat(\n",
    "            model=\"mistral-large-latest\",\n",
    "            messages=[ChatMessage(role=\"user\", content=prompt)],\n",
    "            temperature=0.0,\n",
    "            max_tokens=10\n",
    "        )\n",
    "        return float(response.choices[0].message.content.strip())\n",
    "    except Exception as e:\n",
    "        print(f\"Error en evaluación de similitud: {str(e)}\")\n",
    "        # Fallback: similitud de coseno básica\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        vectorizer = TfidfVectorizer().fit_transform([response_a, response_b])\n",
    "        return ((vectorizer * vectorizer.T).A[0,1] * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fdc755",
   "metadata": {},
   "source": [
    "# Procesamiento de Respuestas\n",
    "\n",
    "El notebook ha sido actualizado para manejar correctamente las respuestas del chatbot que incluyen metadatos. Las principales mejoras son:\n",
    "\n",
    "1. **Extracción de contenido**: Se extrae el contenido real de las respuestas que vienen en formato completo de Mistral\n",
    "2. **Limpieza de respuestas**: Se normalizan los espacios y caracteres especiales\n",
    "3. **Evaluación robusta**: La evaluación de similitud ahora maneja mejor diferentes formatos de respuesta\n",
    "4. **Fallback automático**: Si hay errores en la evaluación principal, se usa una métrica de similitud de coseno como respaldo\n",
    "\n",
    "Esto asegura que la comparación se realice solo sobre el contenido relevante de las respuestas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "006f20cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Chatbot de Mistral AI\n",
    "def get_mistral_response(question):\n",
    "    \"\"\"Obtiene respuesta del modelo Mistral\"\"\"\n",
    "    system_prompt = \"Eres un experto en turismo cubano. Proporciona información precisa y útil sobre Cuba.\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat(\n",
    "            model=\"mistral-large-latest\",\n",
    "            messages=[\n",
    "                ChatMessage(role=\"system\", content=system_prompt),\n",
    "                ChatMessage(role=\"user\", content=question)\n",
    "            ],\n",
    "            temperature=0.3,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error en Mistral: {str(e)}\")\n",
    "        return \"Error: No se pudo generar respuesta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21709d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Análisis estadístico con bootstrapping\n",
    "def analyze_with_bootstrapping(results, n_bootstraps=1000, sample_size=300):\n",
    "    \"\"\"Realiza análisis estadístico con bootstrapping\"\"\"\n",
    "    similarity_scores = [res['similarity'] for res in results]\n",
    "    \n",
    "    # Estadísticas originales (30 preguntas)\n",
    "    original_mean = np.mean(similarity_scores)\n",
    "    original_std = np.std(similarity_scores)\n",
    "    \n",
    "    # Bootstrapping para simular muestras más grandes\n",
    "    bootstrap_means = []\n",
    "    for _ in range(n_bootstraps):\n",
    "        sample = np.random.choice(similarity_scores, size=sample_size, replace=True)\n",
    "        bootstrap_means.append(np.mean(sample))\n",
    "    \n",
    "    # Intervalo de confianza\n",
    "    ci_lower = np.percentile(bootstrap_means, 2.5)\n",
    "    ci_upper = np.percentile(bootstrap_means, 97.5)\n",
    "    \n",
    "    return {\n",
    "        \"original_mean\": original_mean,\n",
    "        \"original_std\": original_std,\n",
    "        \"bootstrap_mean\": np.mean(bootstrap_means),\n",
    "        \"bootstrap_std\": np.std(bootstrap_means),\n",
    "        \"ci_95_lower\": ci_lower,\n",
    "        \"ci_95_upper\": ci_upper,\n",
    "        \"n_bootstraps\": n_bootstraps,\n",
    "        \"sample_size\": sample_size\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "253d7636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función principal de evaluación actualizada\n",
    "def run_evaluation(corpus_path='corpus.json', n_questions=30):\n",
    "    \"\"\"Ejecuta el proceso completo de evaluación\"\"\"\n",
    "    # Verificar si ya existen preguntas\n",
    "    try:\n",
    "        existing_questions = pd.read_csv('preguntas_evaluacion.csv')\n",
    "        if not existing_questions.empty:\n",
    "            questions = existing_questions['pregunta'].tolist()\n",
    "            print(f\"Se encontraron {len(questions)} preguntas existentes en preguntas_evaluacion.csv\")\n",
    "        else:\n",
    "            raise FileNotFoundError\n",
    "    except:\n",
    "        print(\"Generando nuevas preguntas...\")\n",
    "        corpus = load_corpus(corpus_path)\n",
    "        questions = generate_corpus_questions(corpus, n_questions)\n",
    "        pd.DataFrame({'pregunta': questions}).to_csv('preguntas_evaluacion.csv', index=False)\n",
    "        print(f\"Se generaron {len(questions)} preguntas nuevas y se guardaron en preguntas_evaluacion.csv\")\n",
    "        return\n",
    "    \n",
    "    # Cargar respuestas manuales\n",
    "    try:\n",
    "        manual_df = pd.read_csv('resultados_manuales.csv')\n",
    "        if len(manual_df) != len(questions):\n",
    "            raise ValueError(f\"El número de respuestas ({len(manual_df)}) no coincide con el número de preguntas ({len(questions)})\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al cargar resultados_manuales.csv: {str(e)}\")\n",
    "        return\n",
    "    \n",
    "    # Procesar preguntas y obtener similitudes\n",
    "    results = []\n",
    "    total_questions = len(manual_df)\n",
    "    print(\"\\nIniciando evaluación comparativa...\")\n",
    "    \n",
    "    for i, row in manual_df.iterrows():\n",
    "        print(f\"\\nProcesando pregunta {i+1}/{total_questions}\")\n",
    "        question = row['pregunta']\n",
    "        your_response = row['respuesta_manual']\n",
    "        \n",
    "        try:\n",
    "            mistral_response = get_mistral_response(question)\n",
    "            similarity = evaluate_similarity(your_response, mistral_response)\n",
    "            \n",
    "            results.append({\n",
    "                'pregunta': question,\n",
    "                'similarity': similarity\n",
    "            })\n",
    "            \n",
    "            print(f\"Similitud calculada: {similarity:.2f}/10\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error procesando pregunta {i+1}: {str(e)}\")\n",
    "            results.append({\n",
    "                'pregunta': question,\n",
    "                'similarity': 0\n",
    "            })\n",
    "        \n",
    "        # Rate limiting\n",
    "        time.sleep(2.0)\n",
    "    \n",
    "    # Análisis estadístico\n",
    "    analysis = analyze_with_bootstrapping(results)\n",
    "    \n",
    "    # Preparar resultados finales\n",
    "    final_results = {\n",
    "        'preguntas_totales': total_questions,\n",
    "        'similitud_promedio': analysis['original_mean'],\n",
    "        'desviacion_estandar': analysis['original_std'],\n",
    "        'intervalo_confianza_95_inferior': analysis['ci_95_lower'],\n",
    "        'intervalo_confianza_95_superior': analysis['ci_95_upper'],\n",
    "        'resultados_por_pregunta': results\n",
    "    }\n",
    "    \n",
    "    # Guardar resultados\n",
    "    timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    output_file = f'resultados_experimento_{timestamp}.json'\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(final_results, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"\\n✓ Experimento completado\")\n",
    "    print(f\"Resultados guardados en: {output_file}\")\n",
    "    print(f\"\\nEstadísticas:\")\n",
    "    print(f\"- Similitud promedio: {final_results['similitud_promedio']:.2f}/10\")\n",
    "    print(f\"- Desviación estándar: {final_results['desviacion_estandar']:.2f}\")\n",
    "    print(f\"- Intervalo de confianza 95%: [{final_results['intervalo_confianza_95_inferior']:.2f}, {final_results['intervalo_confianza_95_superior']:.2f}]\")\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef768cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Evaluación de similitud semántica\n",
    "def evaluate_similarity(response_a, response_b):\n",
    "    \"\"\"Evalúa la similitud semántica entre dos respuestas\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Como experto en evaluación de similitud semántica, analiza estas dos respuestas sobre turismo en Cuba:\n",
    "    \n",
    "    Respuesta 1: {response_a}\n",
    "    Respuesta 2: {response_b}\n",
    "    \n",
    "    Evalúa la similitud considerando:\n",
    "    1. Precisión de la información\n",
    "    2. Coherencia y relevancia\n",
    "    3. Cobertura del tema\n",
    "    4. Calidad de las recomendaciones\n",
    "    \n",
    "    Asigna una puntuación:\n",
    "    - 0-2: Respuestas totalmente diferentes o contradictorias\n",
    "    - 3-4: Algunas similitudes pero mayormente diferentes\n",
    "    - 5-6: Similitud moderada, información parcialmente compartida\n",
    "    - 7-8: Alta similitud en contenido y enfoque\n",
    "    - 9-10: Prácticamente idénticas en significado y valor informativo\n",
    "    \n",
    "    Devuelve SOLO un número entre 0 y 10.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat(\n",
    "            model=\"mistral-large-latest\",\n",
    "            messages=[ChatMessage(role=\"user\", content=prompt)]\n",
    "            max_tokens=10\n",
    "        )\n",
    "        return float(response.choices[0].message.content.strip())\n",
    "    except:\n",
    "        # Fallback: similitud de coseno básica\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        vectorizer = TfidfVectorizer().fit_transform([response_a, response_b])\n",
    "        return ((vectorizer * vectorizer.T).A[0,1] * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157199b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Flujo principal de evaluación\n",
    "def run_evaluation(corpus_path='corpus.json', n_questions=30):\n",
    "    \"\"\"Ejecuta el proceso completo de evaluación\"\"\"\n",
    "    # Verificar si ya existen preguntas\n",
    "    try:\n",
    "        existing_questions = pd.read_csv('preguntas_evaluacion.csv')\n",
    "        if not existing_questions.empty:\n",
    "            questions = existing_questions['pregunta'].tolist()\n",
    "            print(f\"Se encontraron {len(questions)} preguntas existentes en preguntas_evaluacion.csv\")\n",
    "        else:\n",
    "            raise FileNotFoundError\n",
    "    except:\n",
    "        print(\"Generando nuevas preguntas...\")\n",
    "        corpus = load_corpus(corpus_path)\n",
    "        questions = generate_corpus_questions(corpus, n_questions)\n",
    "        pd.DataFrame({'pregunta': questions}).to_csv('preguntas_evaluacion.csv', index=False)\n",
    "        print(f\"Se generaron {len(questions)} preguntas nuevas y se guardaron en preguntas_evaluacion.csv\")\n",
    "    \n",
    "    # Función para extraer el contenido de la respuesta\n",
    "    def extract_response_content(response_str):\n",
    "        try:\n",
    "            if 'content=' in response_str:\n",
    "                # Buscar el contenido entre content=' y ', name=\n",
    "                start = response_str.find(\"content='\") + 9\n",
    "                end = response_str.find(\"', name=\")\n",
    "                if start > 8 and end > start:  # Si encontramos ambos marcadores\n",
    "                    return response_str[start:end]\n",
    "            return response_str\n",
    "        except:\n",
    "            return response_str\n",
    "\n",
    "    # Si ya tenemos respuestas manuales, continuar con la evaluación\n",
    "    try:\n",
    "        manual_df = pd.read_csv('resultados_manuales.csv')\n",
    "        if len(manual_df) != len(questions):\n",
    "            raise ValueError(\"Número de respuestas no coincide con preguntas\")\n",
    "            \n",
    "        # Procesar las respuestas para extraer solo el contenido\n",
    "        manual_df['respuesta_manual'] = manual_df['respuesta_manual'].apply(extract_response_content)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando respuestas manuales: {str(e)}\")\n",
    "        # Crear plantilla para respuestas manuales\n",
    "        manual_df = pd.DataFrame({\n",
    "            'pregunta': questions,\n",
    "            'respuesta_manual': [''] * len(questions)\n",
    "        })\n",
    "        manual_df.to_csv('resultados_manuales.csv', index=False)\n",
    "        return\n",
    "    \n",
    "    results = []\n",
    "    print(\"\\nIniciando evaluación comparativa con Mistral AI...\")\n",
    "    \n",
    "    for i, row in manual_df.iterrows():\n",
    "        max_retries = 3\n",
    "        retry_delay = 2\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                print(f\"Procesando pregunta {i+1}/{len(manual_df)}\")\n",
    "                question = row['pregunta']\n",
    "                your_response = row['respuesta_manual'].strip()\n",
    "                \n",
    "                if not your_response:\n",
    "                    print(f\"Advertencia: Respuesta manual vacía para pregunta {i+1}\")\n",
    "                    continue\n",
    "                \n",
    "                mistral_response = get_mistral_response(question)\n",
    "                if not mistral_response or mistral_response == \"Error: No se pudo generar respuesta\":\n",
    "                    print(f\"Error: No se pudo obtener respuesta de Mistral para pregunta {i+1}\")\n",
    "                    time.sleep(retry_delay)\n",
    "                    continue\n",
    "                \n",
    "                similarity = evaluate_similarity(your_response, mistral_response)\n",
    "                \n",
    "                results.append({\n",
    "                    'pregunta': question,\n",
    "                    'respuesta_manual': your_response,\n",
    "                    'respuesta_mistral': mistral_response,\n",
    "                    'similarity': similarity\n",
    "                })\n",
    "                \n",
    "                # Rate limiting adaptativo\n",
    "                time.sleep(2.0)  # Base delay\n",
    "                break  # Si llegamos aquí, todo salió bien\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error en pregunta {i+1}, intento {attempt + 1}: {str(e)}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(retry_delay * (attempt + 1))  # Backoff exponencial\n",
    "                else:\n",
    "                    print(f\"Error máximo de intentos para pregunta {i+1}\")\n",
    "    \n",
    "    # Análisis estadístico\n",
    "    analysis = analyze_with_bootstrapping(results)\n",
    "    \n",
    "    # Guardar resultados completos\n",
    "    timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(f\"resultados_completos_{timestamp}.csv\", index=False)\n",
    "    \n",
    "    # Guardar análisis estadístico\n",
    "    analysis_df = pd.DataFrame([analysis])\n",
    "    analysis_df.to_csv(f\"analisis_estadistico_{timestamp}.csv\", index=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"RESULTADOS FINALES\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Similitud promedio (30 preguntas): {analysis['original_mean']:.2f} ± {analysis['original_std']:.2f}\")\n",
    "    print(f\"Similitud estimada ({analysis['sample_size']} preguntas simuladas): {analysis['bootstrap_mean']:.2f}\")\n",
    "    print(f\"Intervalo de confianza 95%: [{analysis['ci_95_lower']:.2f}, {analysis['ci_95_upper']:.2f}]\")\n",
    "    \n",
    "    return results_df, analysis_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56088ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se encontraron 30 preguntas existentes en preguntas_evaluacion.csv\n",
      "\n",
      "Iniciando evaluación comparativa con Mistral AI...\n",
      "Procesando pregunta 1/30\n",
      "Procesando pregunta 2/30\n",
      "Procesando pregunta 2/30\n",
      "Procesando pregunta 3/30\n",
      "Procesando pregunta 3/30\n",
      "Procesando pregunta 4/30\n",
      "Procesando pregunta 4/30\n",
      "Procesando pregunta 5/30\n",
      "Procesando pregunta 5/30\n",
      "Procesando pregunta 6/30\n",
      "Procesando pregunta 6/30\n",
      "Procesando pregunta 7/30\n",
      "Procesando pregunta 7/30\n",
      "Procesando pregunta 8/30\n",
      "Procesando pregunta 8/30\n",
      "Procesando pregunta 9/30\n",
      "Procesando pregunta 9/30\n",
      "Procesando pregunta 10/30\n",
      "Procesando pregunta 10/30\n",
      "Procesando pregunta 11/30\n",
      "Procesando pregunta 11/30\n",
      "Procesando pregunta 12/30\n",
      "Procesando pregunta 12/30\n",
      "Procesando pregunta 13/30\n",
      "Procesando pregunta 13/30\n",
      "Procesando pregunta 14/30\n",
      "Procesando pregunta 14/30\n",
      "Procesando pregunta 15/30\n",
      "Procesando pregunta 15/30\n",
      "Procesando pregunta 16/30\n",
      "Procesando pregunta 16/30\n",
      "Procesando pregunta 17/30\n",
      "Procesando pregunta 17/30\n",
      "Procesando pregunta 18/30\n",
      "Procesando pregunta 18/30\n",
      "Procesando pregunta 19/30\n",
      "Procesando pregunta 19/30\n",
      "Procesando pregunta 20/30\n",
      "Procesando pregunta 20/30\n",
      "Procesando pregunta 21/30\n",
      "Procesando pregunta 21/30\n",
      "Procesando pregunta 22/30\n",
      "Procesando pregunta 22/30\n",
      "Procesando pregunta 23/30\n",
      "Procesando pregunta 23/30\n",
      "Procesando pregunta 24/30\n",
      "Procesando pregunta 24/30\n",
      "Procesando pregunta 25/30\n",
      "Procesando pregunta 25/30\n",
      "Procesando pregunta 26/30\n",
      "Procesando pregunta 26/30\n",
      "Procesando pregunta 27/30\n",
      "Procesando pregunta 27/30\n",
      "Procesando pregunta 28/30\n",
      "Procesando pregunta 28/30\n",
      "Procesando pregunta 29/30\n",
      "Procesando pregunta 29/30\n",
      "Procesando pregunta 30/30\n",
      "Procesando pregunta 30/30\n",
      "\n",
      "==================================================\n",
      "RESULTADOS FINALES\n",
      "==================================================\n",
      "Similitud promedio (30 preguntas): 3.63 ± 1.64\n",
      "Similitud estimada (300 preguntas simuladas): 3.63\n",
      "Intervalo de confianza 95%: [3.46, 3.82]\n",
      "\n",
      "==================================================\n",
      "RESULTADOS FINALES\n",
      "==================================================\n",
      "Similitud promedio (30 preguntas): 3.63 ± 1.64\n",
      "Similitud estimada (300 preguntas simuladas): 3.63\n",
      "Intervalo de confianza 95%: [3.46, 3.82]\n"
     ]
    }
   ],
   "source": [
    "# Ejecutar la evaluación\n",
    "# NOTA: La primera ejecución genera el CSV para respuestas manuales\n",
    "# La segunda ejecución (después de completar el CSV) realiza la evaluación completa\n",
    "results, analysis = run_evaluation(corpus_path='../data/processed/normalized_data.json', n_questions=30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
